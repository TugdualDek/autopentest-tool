from plugins.base_plugin import BasePlugin
import asyncio
import aiohttp
from urllib.parse import urljoin, urlparse, urldefrag, parse_qs, urlencode, urlunparse
import re
import sys
import json
import difflib
import time


class HttpPlugin(BasePlugin):
    def __init__(self):
        super().__init__()
        self.start_url = None
        self.max_depth = 3
        self.max_pages = 100
        self.concurrency = 10
        self.visited = set()
        self.queue = None
        self.session = None
        self.base_domain = None
        self.semaphore = None
        self.start_time = None
        self.links_with_params = set()
        self.all_urls = set()
        self.forms = []
        self.headers = {}
        self.cookies = {}

        self.blacklist_extensions = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp', 'mp4', 'avi', 'mov', 'wmv',
                                     'flv', 'webm', 'mp3', 'wav', 'ogg', 'flac', 'aac', 'pdf', 'doc', 'docx', 'xls',
                                     'xlsx', 'ppt', 'pptx', 'zip', 'rar', '7z', 'tar', 'gz', 'exe', 'msi', 'bin', 'dmg', 'css', 'svg'}

    def evaluate_success_criteria(self, criterion, action_result):
        if criterion == "at_least_one_url_found":
            return action_result and len(action_result) > 0
        elif criterion == "vulnerable_urls_found":
            return action_result and len(action_result) > 0
        elif criterion == "login_page_found":
            return action_result and len(action_result) > 0
        return super().evaluate_success_criteria(criterion, action_result)

    async def crawl_website(self, start_url, max_depth=3, max_pages=100, concurrency=10):
        self.start_url = start_url
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.concurrency = concurrency
        self.base_domain = urlparse(start_url).netloc
        self.semaphore = asyncio.Semaphore(concurrency)
        self.queue = asyncio.Queue()

        try:
            self.session = aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False))
            await self.queue.put((self.start_url, 0))

            tasks = [asyncio.create_task(self.worker()) for _ in range(self.concurrency)]
            await self.queue.join()

            for task in tasks:
                task.cancel()
            await asyncio.gather(*tasks, return_exceptions=True)
        finally:
            await self.session.close()

        print(f"Crawling completed. Found {len(self.all_urls)} unique URLs.")
        return list(self.all_urls)

    async def worker(self):
        while True:
            url, depth = await self.queue.get()
            try:
                await self.process_url(url, depth)
            except asyncio.CancelledError:
                break
            finally:
                self.queue.task_done()

    async def process_url(self, url, depth):
        if url in self.visited or len(self.visited) >= self.max_pages or depth > self.max_depth:
            return

        self.visited.add(url)
        self.all_urls.add(url)

        async with self.semaphore:
            try:
                async with self.session.get(url, timeout=10) as response:
                    if response.status == 200:
                        content_type = response.headers.get('Content-Type', '')
                        if 'text/html' in content_type:
                            text = await response.text()
                            new_urls = self.extract_links(url, text)
                            for new_url in new_urls:
                                if new_url not in self.visited:
                                    await self.queue.put((new_url, depth + 1))
            except Exception as e:
                print(f"Error crawling {url}: {str(e)}")

    def is_valid_url(self, url):
        parsed = urlparse(url)
        return parsed.netloc == self.base_domain and parsed.scheme in ('http', 'https')

    def extract_links(self, base_url, html):
        links = set()
        href_pattern = re.compile(r'href=[\'"]?([^\'" >]+)')
        for match in href_pattern.finditer(html):
            href = match.group(1)
            full_url = urljoin(base_url, href)
            full_url, _ = urldefrag(full_url)
            if self.is_valid_url(full_url):
                links.add(full_url)
        return links

    def is_valid_url(self, url):
        url, _ = urldefrag(url)
        parsed = urlparse(url)

        if parsed.netloc != self.base_domain or parsed.scheme not in ('http', 'https'):
            return False

        path = parsed.path.lower()
        extension = path.split('.')[-1] if '.' in path else ''
        if extension in self.blacklist_extensions:
            return False

        if url in self.visited:
            return False

        if '<%' in url or '%>' in url:
            return False

        return True

    def extract_forms(self, url, html):
        form_pattern = re.compile(r'<form.*?</form>', re.DOTALL)
        input_pattern = re.compile(r'<input.*?>', re.DOTALL)
        for form_match in form_pattern.finditer(html):
            form_html = form_match.group(0)
            action = re.search(r'action=[\'"]?([^\'" >]+)', form_html)
            method = re.search(r'method=[\'"]?([^\'" >]+)', form_html)
            form_data = {
                'url': urljoin(url, action.group(1) if action else ''),
                'method': method.group(1).upper() if method else 'GET',
                'inputs': []
            }
            for input_match in input_pattern.finditer(form_html):
                input_html = input_match.group(0)
                name = re.search(r'name=[\'"]?([^\'" >]+)', input_html)
                input_type = re.search(r'type=[\'"]?([^\'" >]+)', input_html)
                value = re.search(r'value=[\'"]?([^\'" >]+)', input_html)
                if name:
                    input_data = {
                        'name': name.group(1),
                        'type': input_type.group(1) if input_type else 'text',
                        'value': value.group(1) if value else ''
                    }
                    form_data['inputs'].append(input_data)
            self.forms.append(form_data)

    async def cleanup(self):
        if self.session:
            await self.session.close()
        tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
        for task in tasks:
            task.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)

    async def test_sql_injection(self, urls, payloads):
        vulnerable_urls = []
        #print(f"Testing {len(urls)} URLs for SQL injection vulnerabilities")

        async with aiohttp.ClientSession() as session:
            for url in urls:
                parsed_url = self.parse_url(url)
                if not parsed_url['query']:
                    continue

                for param, value in parsed_url['query'].items():
                    for payload in payloads:
                        try:
                            # Test de base
                            test_url = self.build_url(self.inject_payload(parsed_url, param, payload))
                            response = await self.make_request(session, test_url)

                            # Vérification des erreurs SQL
                            if self.check_sql_errors(response['content']):
                                vulnerable_urls.append(
                                    {"url": url, "payload": payload, "param": param, "type": "error-based"})
                                break

                            # Test d'injection à l'aveugle basé sur le temps
                            time_based_payload = f"{payload} AND SLEEP(5)"
                            time_test_url = self.build_url(self.inject_payload(parsed_url, param, time_based_payload))
                            start_time = time.time()
                            await self.make_request(session, time_test_url)
                            elapsed_time = time.time() - start_time

                            if elapsed_time > 5:
                                vulnerable_urls.append(
                                    {"url": url, "payload": time_based_payload, "param": param, "type": "time-based"})
                                break

                            # Test d'injection basé sur le contenu
                            true_payload = f"{payload} AND 1=1"
                            false_payload = f"{payload} AND 1=2"
                            true_url = self.build_url(self.inject_payload(parsed_url, param, true_payload))
                            false_url = self.build_url(self.inject_payload(parsed_url, param, false_payload))

                            true_response = await self.make_request(session, true_url)
                            false_response = await self.make_request(session, false_url)

                            if self.compare_responses(true_response['content'], false_response['content']):
                                vulnerable_urls.append(
                                    {"url": url, "payload": payload, "param": param, "type": "boolean-based"})
                                break

                        except Exception as e:
                            print(f"Error testing SQL injection on {url}, param {param}: {str(e)}")

        # print(f"Found {len(vulnerable_urls)} vulnerable URLs")
        return vulnerable_urls

    def inject_payload(self, parsed_url, param, payload):
        new_parsed_url = parsed_url.copy()
        new_parsed_url['query'][param] = [payload]
        return new_parsed_url

    def check_sql_errors(self, content):
        error_patterns = [
            "SQL syntax.*MySQL", "Warning.*mysql_.*", "valid MySQL result", "MySqlClient\.",
            "PostgreSQL.*ERROR", "Warning.*\Wpg_.*", "valid PostgreSQL result", "Npgsql\.",
            "Driver.* SQL[\-\_\ ]*Server", "OLE DB.* SQL Server", "(\W|\A)SQL Server.*Driver",
            "Warning.*mssql_.*", "(\W|\A)SQL Server.*[0-9a-fA-F]{8}", "(?s)Exception.*\WSystem\.Data\.SqlClient\.",
            "Oracle.*Driver", "Warning.*\Woci_.*", "Warning.*\Wora_.*"
        ]
        return any(re.search(pattern, content, re.IGNORECASE) for pattern in error_patterns)

    def compare_responses(self, response1, response2):
        # Comparer les longueurs des réponses
        if abs(len(response1) - len(response2)) > 10:
            return True

        # Comparer le contenu des réponses
        diff_ratio = difflib.SequenceMatcher(None, response1, response2).ratio()
        return diff_ratio < 0.95

    def has_parameters(self, url):
        parsed_url = urlparse(url)
        return bool(parse_qs(parsed_url.query))

    def run(self, start_url, max_depth=3, max_pages=100, concurrency=10):
        if sys.platform.startswith('win'):
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        return asyncio.run(self.crawl_website(start_url, max_depth, max_pages, concurrency))

    async def make_request(self, session, url):
        if not url or not url.startswith('http'):
            raise ValueError(f"Invalid URL: {url}")

        async with session.get(url) as response:
            return {
                'content': await response.text(),
                'status': response.status
            }

    def parse_url(self, url):
        parsed = urlparse(url)
        return {
            'scheme': parsed.scheme,
            'netloc': parsed.netloc,
            'path': parsed.path,
            'params': parsed.params,
            'query': parse_qs(parsed.query),
            'fragment': parsed.fragment
        }

    def build_url(self, parsed_url):
        query = urlencode(parsed_url['query'], doseq=True)
        return urlunparse((
            parsed_url['scheme'],
            parsed_url['netloc'],
            parsed_url['path'],
            parsed_url['params'],
            query,
            parsed_url['fragment']
        ))

    def encode_params(self, params):
        return urlencode(params)

    def decode_params(self, encoded_params):
        return parse_qs(encoded_params)

    def parse_json(self, json_string):
        try:
            return json.loads(json_string)
        except json.JSONDecodeError:
            return None

    def to_json(self, data):
        return json.dumps(data)

    def find_login_page(self, urls):
        login_pages = []

        # find urls that corresponds to login pages (login.php, login.html, ...
        for url in urls:
            parsed_url = urlparse(url)
            if 'login' or 'connexion' or 'connection' or 'signin' in parsed_url.path:
                login_pages.append(url)

        return login_pages
